{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fe26726-1657-4702-b9ed-280da429625f",
   "metadata": {},
   "source": [
    "# IT3103 Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ffba2-595d-49bf-8a00-0c44ce45b170",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e24dc950-3977-44c7-b773-433c87197e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdef865-c1fa-4137-b59c-4ecd58e5c9f8",
   "metadata": {},
   "source": [
    "### Importing the dataset\n",
    "Imports the datasets to be used when training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ff138b-6fe4-417c-bb68-d7406b7c4665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file from the link\n",
    "dataset_URL = 'https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/it3103/datasets/fruits.zip'\n",
    "\n",
    "# Extract the file into a folder\n",
    "path_to_zip = tf.keras.utils.get_file('fruits.zip', origin=dataset_URL, extract=True, cache_dir='.')\n",
    "\n",
    "# Set the path to look for the files\n",
    "dataset_dir = os.path.join(os.path.dirname(path_to_zip), \"fruits_extracted/fruits\")\n",
    "dataset_dir_valid = os.path.join(os.path.dirname(path_to_zip), \"fruits_extracted/fruits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1a970-b733-4ef6-8a91-2c83a8e93d89",
   "metadata": {},
   "source": [
    "### Extracting datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99b6a014-a5b4-4f9b-b9e6-1c991385bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Directory\n",
    "trainingDirectory = os.path.join(dataset_dir, \"train\")\n",
    "\n",
    "# Validation Directory\n",
    "validationDirectory = os.path.join(dataset_dir_valid, \"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15383c45-20e1-4c6e-a92e-e5032a938467",
   "metadata": {},
   "source": [
    "### Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63676679-312a-48db-bd71-0a53c5f0ed1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      " random_flip (RandomFlip)    (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      " random_rotation (RandomRota  (None, 128, 128, 3)      0         \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " random_zoom (RandomZoom)    (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 126, 126, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 63, 63, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 61, 61, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 30, 30, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               2359808   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,733,510\n",
      "Trainable params: 2,733,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def make_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Input(shape=(128,128, 3)))\n",
    "    model.add(keras.layers.Rescaling(1./255))\n",
    "\n",
    "    # Data Augmentation\n",
    "    model.add(keras.layers.RandomFlip(\"horizontals_and_vertical\"))\n",
    "    model.add(keras.layers.RandomRotation(0.3))\n",
    "    model.add(keras.layers.RandomZoom(0.3))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "    model.add(keras.layers.Conv2D(128, (3, 3), activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "    model.add(keras.layers.Conv2D(128, (3, 3), activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(512, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(256, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(6, activation=\"softmax\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = make_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7caad6-0909-491c-b3e1-44ea25a4150e",
   "metadata": {},
   "source": [
    "### Adding Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3470764-c213-48ef-8511-928eb8deba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse Categorical Crossentropy is used when truth labels are integer coded and have multiple classes\n",
    "# Categorical Crossentropy is used when it is one hot coded\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279ea6c-e804-443e-96d1-ff8674fc85b4",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8aefd32-4ad7-4a2a-a642-17c6eb522cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1182 files belonging to 6 classes.\n",
      "Found 329 files belonging to 6 classes.\n",
      "Images Shape:  (32, 128, 128, 3)\n",
      "Labels Shape:  (32,)\n",
      "tf.Tensor([2 3 0 3 4 3 4 4 1 2 4 0 4 1 2 1 3 5 1 3 2 5 1 3 4 3 3 0 3 0 0 0], shape=(32,), dtype=int32)\n",
      "['freshapples', 'freshbanana', 'freshoranges', 'rottenapples', 'rottenbanana', 'rottenoranges']\n",
      "['freshapples', 'freshbanana', 'freshoranges', 'rottenapples', 'rottenbanana', 'rottenoranges']\n"
     ]
    }
   ],
   "source": [
    "imgHeight, imgWidth = 128, 128\n",
    "batchSize = 32\n",
    "\n",
    "# Resizing all images into the smae size\n",
    "imageSize = (imgHeight, imgWidth)\n",
    "\n",
    "trainingDataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    trainingDirectory,\n",
    "    seed=1,\n",
    "    image_size=imageSize,\n",
    "    batch_size=batchSize,\n",
    "    label_mode=\"int\"\n",
    ")\n",
    "\n",
    "validationDataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    validationDirectory,\n",
    "    seed=1,\n",
    "    image_size=imageSize,\n",
    "    batch_size=batchSize,\n",
    "    label_mode=\"int\"\n",
    ")\n",
    "\n",
    "for images, labels in trainingDataset.take(1):\n",
    "    print(\"Images Shape: \", images.shape)\n",
    "    print(\"Labels Shape: \", labels.shape)\n",
    "    print(tf.squeeze(labels))\n",
    "\n",
    "# print out the indices to find out the class\n",
    "print(trainingDataset.class_names)\n",
    "print(validationDataset.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780233c1-af70-496c-bc21-5e658ea821c1",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae8b754-516f-4fad-ba3c-24bc57cceef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "37/37 [==============================] - 8s 115ms/step - loss: 1.7355 - accuracy: 0.2741 - val_loss: 1.5836 - val_accuracy: 0.3860\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 4s 101ms/step - loss: 1.4909 - accuracy: 0.4010 - val_loss: 1.2541 - val_accuracy: 0.4833\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 4s 113ms/step - loss: 1.2966 - accuracy: 0.4492 - val_loss: 1.1197 - val_accuracy: 0.5927\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 4s 110ms/step - loss: 1.1521 - accuracy: 0.5212 - val_loss: 1.0032 - val_accuracy: 0.6079\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 4s 118ms/step - loss: 1.0755 - accuracy: 0.5685 - val_loss: 1.0543 - val_accuracy: 0.6109\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 4s 115ms/step - loss: 0.9830 - accuracy: 0.6159 - val_loss: 0.8377 - val_accuracy: 0.6869\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 4s 113ms/step - loss: 0.8897 - accuracy: 0.6557 - val_loss: 0.7964 - val_accuracy: 0.6869\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 4s 114ms/step - loss: 0.8189 - accuracy: 0.6878 - val_loss: 0.7676 - val_accuracy: 0.7112\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 4s 118ms/step - loss: 0.7877 - accuracy: 0.6980 - val_loss: 0.7229 - val_accuracy: 0.7508\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 4s 113ms/step - loss: 0.7156 - accuracy: 0.7276 - val_loss: 0.6991 - val_accuracy: 0.7447\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 4s 115ms/step - loss: 0.6750 - accuracy: 0.7513 - val_loss: 0.6269 - val_accuracy: 0.8176\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 4s 113ms/step - loss: 0.6484 - accuracy: 0.7758 - val_loss: 0.5821 - val_accuracy: 0.7964\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 5s 120ms/step - loss: 0.5848 - accuracy: 0.7970 - val_loss: 0.5005 - val_accuracy: 0.8450\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 4s 114ms/step - loss: 0.5604 - accuracy: 0.8046 - val_loss: 0.5892 - val_accuracy: 0.8085\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 4s 115ms/step - loss: 0.5226 - accuracy: 0.8257 - val_loss: 0.4815 - val_accuracy: 0.8571\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 4s 115ms/step - loss: 0.4840 - accuracy: 0.8283 - val_loss: 0.4663 - val_accuracy: 0.8359\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 5s 120ms/step - loss: 0.4627 - accuracy: 0.8486 - val_loss: 0.4368 - val_accuracy: 0.8693\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 4s 115ms/step - loss: 0.4426 - accuracy: 0.8477 - val_loss: 0.4140 - val_accuracy: 0.8663\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 4s 118ms/step - loss: 0.4428 - accuracy: 0.8435 - val_loss: 0.4393 - val_accuracy: 0.8511\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.4255 - accuracy: 0.8553 - val_loss: 0.3940 - val_accuracy: 0.8693\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 4s 118ms/step - loss: 0.4387 - accuracy: 0.8409 - val_loss: 0.4818 - val_accuracy: 0.8450\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 4s 115ms/step - loss: 0.4133 - accuracy: 0.8460 - val_loss: 0.3911 - val_accuracy: 0.8541\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.3972 - accuracy: 0.8519 - val_loss: 0.3968 - val_accuracy: 0.8632\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.3496 - accuracy: 0.8723 - val_loss: 0.4915 - val_accuracy: 0.8571\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 4s 117ms/step - loss: 0.4107 - accuracy: 0.8545 - val_loss: 0.4216 - val_accuracy: 0.8754\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 0.3346 - accuracy: 0.8790 - val_loss: 0.3608 - val_accuracy: 0.8845\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.3451 - accuracy: 0.8858 - val_loss: 0.4317 - val_accuracy: 0.8723\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 0.3432 - accuracy: 0.8680 - val_loss: 0.4277 - val_accuracy: 0.8723\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.3348 - accuracy: 0.8816 - val_loss: 0.4023 - val_accuracy: 0.8511\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 5s 121ms/step - loss: 0.3331 - accuracy: 0.8841 - val_loss: 0.3614 - val_accuracy: 0.8723\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.3163 - accuracy: 0.8832 - val_loss: 0.4088 - val_accuracy: 0.8632\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 5s 130ms/step - loss: 0.3121 - accuracy: 0.8900 - val_loss: 0.4089 - val_accuracy: 0.8845\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.2951 - accuracy: 0.8951 - val_loss: 0.3762 - val_accuracy: 0.8845\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 5s 130ms/step - loss: 0.2947 - accuracy: 0.8917 - val_loss: 0.3382 - val_accuracy: 0.8815\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 5s 119ms/step - loss: 0.2992 - accuracy: 0.8883 - val_loss: 0.3610 - val_accuracy: 0.8754\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.2704 - accuracy: 0.9086 - val_loss: 0.3647 - val_accuracy: 0.8663\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 5s 131ms/step - loss: 0.2695 - accuracy: 0.9044 - val_loss: 0.3257 - val_accuracy: 0.8875\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.2819 - accuracy: 0.8934 - val_loss: 0.3338 - val_accuracy: 0.8875\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.2893 - accuracy: 0.8959 - val_loss: 0.3463 - val_accuracy: 0.8906\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 4s 116ms/step - loss: 0.2705 - accuracy: 0.9036 - val_loss: 0.3593 - val_accuracy: 0.8967\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.2455 - accuracy: 0.9061 - val_loss: 0.3372 - val_accuracy: 0.8936\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 0.2606 - accuracy: 0.9010 - val_loss: 0.3242 - val_accuracy: 0.8967\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 5s 120ms/step - loss: 0.2417 - accuracy: 0.9095 - val_loss: 0.3846 - val_accuracy: 0.8967\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 5s 120ms/step - loss: 0.2754 - accuracy: 0.8934 - val_loss: 0.3287 - val_accuracy: 0.8967\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 5s 130ms/step - loss: 0.2322 - accuracy: 0.9137 - val_loss: 0.3518 - val_accuracy: 0.8967\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.2439 - accuracy: 0.9171 - val_loss: 0.3141 - val_accuracy: 0.8845\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 4s 118ms/step - loss: 0.2416 - accuracy: 0.9103 - val_loss: 0.3828 - val_accuracy: 0.8936\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.2450 - accuracy: 0.9069 - val_loss: 0.3349 - val_accuracy: 0.8906\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.2217 - accuracy: 0.9162 - val_loss: 0.3282 - val_accuracy: 0.8967\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 5s 121ms/step - loss: 0.2702 - accuracy: 0.9027 - val_loss: 0.3744 - val_accuracy: 0.8815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25a4970b130>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_tb_callback():\n",
    "    root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
    "\n",
    "    def get_run_logdir():\n",
    "        import time\n",
    "        run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "        return os.path.join(root_logdir, run_id)\n",
    "\n",
    "    run_logdir = get_run_logdir()\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "    return tb_callback\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"bestcheckpoint.weights.h5\",\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    trainingDataset,\n",
    "    epochs=50,\n",
    "    validation_data=validationDataset,\n",
    "    callbacks=[create_tb_callback(), model_checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16fb0134-f160-4d31-a695-3683eed55508",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(model, filepath=\"cur_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864d47e1-5d1a-48a7-b454-24d1bf135940",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d31ab80-b28f-452b-b0fc-1fabaa9225bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freshapples\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "\n",
    "filename = \"TestImages/apple.jpg\"\n",
    "\n",
    "test_image = keras.preprocessing.image.load_img(\n",
    "    filename, target_size=(128, 128)\n",
    ")\n",
    "\n",
    "# Converts the image to numpy array\n",
    "img_array = keras.preprocessing.image.img_to_array(test_image)\n",
    "\n",
    "img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "# Loading the model  to do the inference\n",
    "model = tf.keras.models.load_model(\"cur_model.keras\")\n",
    "predicted_label = model(img_array)\n",
    "\n",
    "class FruitType(Enum):\n",
    "    freshapples = 0\n",
    "    freshbanana = 1\n",
    "    freshoranges = 2\n",
    "    rottenapples = 3\n",
    "    rottenbanana = 4\n",
    "    rottenoranges = 5\n",
    "\n",
    "print(FruitType(np.argmax(predicted_label)).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a49150d2-207a-423b-bb33-e38e97c66a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['freshapples',\n",
       " 'freshbanana',\n",
       " 'freshoranges',\n",
       " 'rottenapples',\n",
       " 'rottenbanana',\n",
       " 'rottenoranges']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validationDataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45997a88-68cb-404a-918a-3ac543296e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
